Transform class basically store's a Node's transform matrix. So this matrix would transform from the local coords to the world coords.
TransformTo takes a point and transforms it to the object's local space.
TransformFrom does the opposite.
We subtract the translation from the point first since that's the inverse of that, then we do the linear transformation. So we are just walking backwards in steps since to go from local to world coords, you do scale, then rotation, then translation. So to do the opposite, we do the reverse translation, the reverse rotation, and the reverse scale.
The way that ToNodeCoords transforms the ray direction is by taking a point that's one unit along the direction of the ray with (ray.p + ray.dir), applying the transformation to that point, then subtractig TransformTo(ray.p) from it to negate the addition in the new space. This cancels the translation part.
TransformTo is for points. world->local. Includes translation and (inverse) linear transformation.
ToNodeCoords transforms a point plus a dir, the subtracts the transformed origin to negate the translation so only the inverse linear transformation actually applies to it.
VectorTransformTo is for transforming normals, not ray directions.
TransformTo(ray.p + ray.dir) - TransformTo(ray.p) = itm * ray.dir
The reason that normals are different from directions is that normals are constrained to be perpendicular to the surfaces, directions vectors are not.
When we do TransformTo, we also subtract the Node's position from the direction, we don't want this since we don't care about the "position" of a direction, so translating it actually messes stuff up. So to undo this and still be able to use TransformTo, we first add the ray position to it, then once it's done having TransformTo applied to it, we subtract the transformed ray position from it since that will not only undo adding the unmodified ray position to the unmodified direction, but will also undo subtracting the Node's position from the direction since the transformed ray pos also had this subtraction done to it, so subtracting that transformed ray pos actually negates this operation?

Vertex normals can be rotated just fine. Translation obviously messes stuff up. Any non-uniform scale will mess stuff up a lot. Can make it not normal to the surface anymore. The constraint on normals is that they need to be perpendicular to the surface. Direction vectors don't have this issue, so we don't have to worry about that. May still need to normalize direction.
Because the normals go in the opposite direction to how they're supposed to go in the non-uniform scale, we need to apply the inverse of this. Think: Imagine we have a normal near the top (but not exactly at the top) of a circle, pointing mostly upwards, like it should. We then squash the circle so that it looks more horizontal. This normal would also get squashed and become more horizontal. This is the opposite direction to where they should go. You would expect that as the circle becomes more flat, the normal should point more upwards, but the opposite happened. So we need to apply the inverse of the non-uniform transform.
The reason this works is because when we do a non-uniform transform, [s_x, s_y, s_z], to a vector, v, you get v'=[v_x*s_x, v_y*s_y, v_z*s_z]. Now, if you apply the inverse of the non-uniform scale to a vector, in this case the surface normal, n, you get n'=[n_x/s_x, n_y/s_y, n_z/s_z]. Now, imagine we have a tangent vector, t. We know that dot(n,t)=0. So if we do dot([n_x/s_x, n_y/s_y, n_z/s_z], [t_x/s_x, t_y/s_y, t_z/s_z]), then obviously the scale parts just negate each other and we end up with 0. After the inverse transform, still need to normalize this normal since you're still scaling the normal so of course it won't be a unit vector. But we do know that this vector will at least be perpendicular to the surface. With homogeneous coordinates, we just use 0 as the \alpha value for normals so that will remove the translation component.
To undo the scaling from a transform matrix, first make it 3x3, then take the inverse then the transpose. This works because these 3x3 matrices can be broken down into R_2 * S * R_1. Take the inverse to get inverse(R_1) * inverse(S) * inverse(R_2), which is equivalent to transpose(R_1) * inverse(S) * transpose(R_2) since the rotation matrices are orthogonal. Then you take the transpose of that entire thing to get R_2 * transpose(inverse(S)) * R_1. But since S is a diagonal matrix, we really just have R_2 * inverse(S) * R_1.
Why can any 3x3 matrix be decomposed into R_2 * S * R_1? Just think of it geometrically and it makes total sense. These are always linear transformations so it makes sense that you can do that. This is called singular value decomposition. Really, R_1 and R_2 just need to be orthogonal matrices and S just needs to be a diagonal matrix with non-negative values.

Reason why an orthogonal matrix's (which we sometimes call an orthonormal matrix, but that terminonlogy is wrong since the people who made this are fucking retarded I guess) transpose is equal to the inverse is this:
If we have an orthogonal matrix, Q, then inverse(Q) * Q = I. Now, if we do transpose(Q) * Q, then we're dotting the columns of Q against the columns of Q. This obvioulsy results in I. This is because when a normal vectors dots itself, you just get 1, and when it dots the other vectors in the matrix, which it's perpendicular to, you'll get 0. And if we do Q * transpose(Q), then we're dotting the rows with the rows: same effect.
The definition of an orthogonal matrix is really that traspose(Q) * Q = I. So you dot the columns with each other and get the identity. The reason that we know that the rows are also orthonormal is because if we're saying that transpose(Q) * Q = I, then transpose(Q) = inverse(Q), and when we have a matrix, Q, then the order with inverse(Q) * Q and Q * inverse(Q) both results in identity anyways.
Think of the columns of a transform matrix as being us drawing the new basis axes in world space, and the rows as being responsible for actually getting the objects there (since we dot them with the points/vectors that are being transformed).

The model matrix has the object's basis vectors expressed in world space.
The view matrix has the world's basis vectors expressed in view space.
So the camera matrix is used to go from camera space to world space. It's columns are the camera basis vectors expressed in world space.
So the idea about the basis vectors in the columns was right the whole time, I was just thinking of it kind of weird.
When we do the model transform, we express the object's basis vectors in world space.
Then we express the world's basis vectors in camera/view space.
The inverse of the view matrix (going from camera to world space) has the camera's basis vectors in world space.
So to go from space A to space B, you make a matrix where the where the basis vectors of A are expressed from space B as the columns of a matrix.
How to think of tangent. Ratio of angles between and adjacent. If the angle is huge (closer to 90 degrees), then the opposite side will obviously be super long so the tangent will be really big. If it's at a 45 degree angle, then they're the same and the tangent is 1.
Imagine that you have the image plane, and the camera is looking at it. Draw a line from the camera to the top and bottom of the plane. This makes an isosceles triangle. The apex angle (angle at the tip) is the fov. To get the horizontal version of tanY, tanX, you just multiply tanY by the aspectRatio.
World space height of camera image plane: 2*tan(fov/2).
World space width of camera image plane: aspectRatio*h

If the camera looked down it's target direction and shot rays in that direction, then it would be looking down its positive z axis.
We want it to look down its negative z axis. We want it to shoot rays along its -z direction. We want the z coord for the ray dir in camera space to always be -1. So we have to negate the z axis when making the camera matrix which takes us from camera space to world space
